# COMPLETELY GENERATED BY CLAUDE TO GET DATA FASTER. CAN BE EDITED TO GET ALL TYPE OF FULLERENES DOWNLOADABLE LINKS LATER.

#!/usr/bin/env python3
"""
Selenium-Based Fullerene XYZ File Scraper for nanotube.msu.edu

This script uses Selenium to interact with the fullerene database website like a real browser,
clicking on images to discover XYZ download links that might be dynamically generated.

Requirements:
    pip install selenium beautifulsoup4 requests
    Download ChromeDriver: https://chromedriver.chromium.org/

Author: AI Assistant
Date: 2025
"""

import os
import time
import json
import csv
import re
from pathlib import Path
from typing import List, Dict, Set, Optional
import logging
from urllib.parse import urljoin, urlparse

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import (
    TimeoutException,
    NoSuchElementException,
    WebDriverException,
)

# For fallback requests
import requests
from bs4 import BeautifulSoup


class SeleniumFullereneScraper:
    def __init__(
        self,
        base_url: str = "https://nanotube.msu.edu/fullerene/",
        headless: bool = True,
        output_dir: str = "fullerene_data",
        chromedriver_path: Optional[str] = None,
    ):
        """
        Initialize the Selenium-based fullerene scraper

        Args:
            base_url: Base URL of the fullerene database
            headless: Run browser in headless mode
            output_dir: Directory to save output files
            chromedriver_path: Path to ChromeDriver executable (auto-detect if None)
        """
        self.base_url = base_url
        self.headless = headless
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.chromedriver_path = chromedriver_path

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(self.output_dir / "selenium_scraper.log"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

        # Storage for found files
        self.xyz_files: List[Dict] = []
        self.carbon_numbers: Set[int] = set()

        # Selenium driver
        self.driver: Optional[webdriver.Chrome] = None

        # Fallback session for requests
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )

    def setup_driver(self) -> webdriver.Chrome:
        """Setup Chrome WebDriver with appropriate options"""
        chrome_options = Options()

        if self.headless:
            chrome_options.add_argument("--headless")

        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")  # Faster loading
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        try:
            if self.chromedriver_path:
                service = Service(self.chromedriver_path)
                driver = webdriver.Chrome(service=service, options=chrome_options)
            else:
                # Try to auto-detect ChromeDriver
                driver = webdriver.Chrome(options=chrome_options)

            driver.set_page_load_timeout(30)
            driver.implicitly_wait(10)

            self.logger.info("Chrome WebDriver initialized successfully")
            return driver

        except Exception as e:
            self.logger.error(f"Failed to initialize Chrome WebDriver: {e}")
            self.logger.error(
                "Please ensure ChromeDriver is installed and in PATH, or provide the path explicitly"
            )
            raise

    def discover_carbon_numbers(self) -> Set[int]:
        """
        Discover available carbon numbers from the main pages using Selenium
        """
        self.logger.info("Discovering available carbon numbers using Selenium...")
        carbon_numbers = set()

        try:
            # Start with the main fullerene page
            main_url = urljoin(self.base_url, "fullerene-isomers.html")
            self.driver.get(main_url)

            # Wait for page to load
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # Look for carbon number links in the page
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, "html.parser")

            # Find links to carbon-specific pages
            for link in soup.find_all("a", href=True):
                href = link["href"]
                match = re.search(r"fullerene\.php\?C=(\d+)", href)
                if match:
                    carbon_numbers.add(int(match.group(1)))

            # Also check the main fullerene.php page for a list
            try:
                fullerene_main = urljoin(self.base_url, "fullerene.php")
                self.driver.get(fullerene_main)
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )

                # Look for clickable fullerene elements
                page_source = self.driver.page_source

                # Extract carbon numbers from various patterns in the page
                carbon_patterns = re.findall(r"C(\d+)", page_source)
                for carbon_str in carbon_patterns:
                    carbon_num = int(carbon_str)
                    if 20 <= carbon_num <= 200:  # Reasonable range for fullerenes
                        carbon_numbers.add(carbon_num)

            except Exception as e:
                self.logger.warning(f"Could not access main fullerene page: {e}")

        except Exception as e:
            self.logger.error(f"Error discovering carbon numbers: {e}")

        # If we didn't find many, try known carbon numbers
        if len(carbon_numbers) < 5:
            self.logger.info("Testing known carbon numbers...")
            known_carbons = [
                20,
                22,
                24,
                26,
                28,
                30,
                32,
                34,
                36,
                38,
                40,
                42,
                44,
                46,
                48,
                50,
                52,
                54,
                56,
                58,
                60,
                62,
                64,
                66,
                68,
                70,
                72,
                74,
                76,
                78,
                80,
                82,
                84,
                86,
                88,
                90,
                92,
                94,
                96,
                98,
                100,
            ]

            for carbon in known_carbons:
                try:
                    url = urljoin(self.base_url, f"fullerene.php?C={carbon}")
                    self.driver.get(url)

                    # Check if page loaded successfully and contains fullerene data
                    if "Total strain energy" in self.driver.page_source:
                        carbon_numbers.add(carbon)
                        self.logger.info(f"Found carbon number: C{carbon}")

                    time.sleep(1)  # Brief pause between requests

                except Exception as e:
                    self.logger.debug(f"C{carbon} not available: {e}")
                    continue

        self.carbon_numbers = carbon_numbers
        self.logger.info(
            f"Discovered {len(carbon_numbers)} carbon numbers: {sorted(carbon_numbers)}"
        )
        return carbon_numbers

    def extract_xyz_links_from_page(self, carbon_num: int) -> List[Dict]:
        """
        Extract XYZ download links from a carbon-specific page by interacting with it
        """
        self.logger.info(f"Extracting XYZ links for C{carbon_num}")
        found_files = []

        try:
            # Navigate to the carbon-specific page
            url = urljoin(self.base_url, f"fullerene.php?C={carbon_num}")
            self.driver.get(url)

            # Wait for page to load
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # Look for images or clickable elements that might lead to XYZ files
            clickable_elements = []

            # Find images that might be clickable
            images = self.driver.find_elements(By.TAG_NAME, "img")
            for img in images:
                try:
                    # Check if image is clickable
                    if img.is_enabled() and img.is_displayed():
                        clickable_elements.append(("img", img))
                except:
                    continue

            # Find links that might contain XYZ files
            links = self.driver.find_elements(By.TAG_NAME, "a")
            for link in links:
                try:
                    href = link.get_attribute("href")
                    if href and (".xyz" in href or "coordinate" in href.lower()):
                        found_files.append(
                            {
                                "carbon_number": carbon_num,
                                "url": href,
                                "filename": os.path.basename(href),
                                "discovery_method": "direct_link",
                                "symmetry": "unknown",
                                "isomer": 0,
                            }
                        )
                        self.logger.info(f"Found direct XYZ link: {href}")
                except:
                    continue

            # Try clicking on images to see if they reveal XYZ links
            for element_type, element in clickable_elements:
                try:
                    # Get current URL before clicking
                    current_url = self.driver.current_url

                    # Scroll to element and click
                    self.driver.execute_script(
                        "arguments[0].scrollIntoView(true);", element
                    )
                    time.sleep(1)

                    # Try to click the element
                    element.click()
                    time.sleep(2)  # Wait for any redirect or download

                    # Check if URL changed (might indicate download or new page)
                    new_url = self.driver.current_url
                    if new_url != current_url:
                        if ".xyz" in new_url:
                            # Extract metadata from URL
                            filename = os.path.basename(new_url)
                            symmetry, isomer = self.parse_filename_metadata(filename)

                            found_files.append(
                                {
                                    "carbon_number": carbon_num,
                                    "url": new_url,
                                    "filename": filename,
                                    "discovery_method": "click_navigation",
                                    "symmetry": symmetry,
                                    "isomer": isomer,
                                }
                            )
                            self.logger.info(f"Found XYZ file via click: {new_url}")

                        # Navigate back to the original page
                        self.driver.get(url)
                        WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )

                except Exception as e:
                    self.logger.debug(f"Error clicking element for C{carbon_num}: {e}")
                    # Try to get back to the original page
                    try:
                        self.driver.get(url)
                    except:
                        pass
                    continue

            # Also check page source for any hidden or dynamically loaded links
            page_source = self.driver.page_source
            xyz_patterns = re.findall(
                r'href=["\']([^"\']*\.xyz[^"\']*)["\']', page_source
            )
            for xyz_url in xyz_patterns:
                if not xyz_url.startswith("http"):
                    xyz_url = urljoin(self.base_url, xyz_url)

                filename = os.path.basename(xyz_url)
                symmetry, isomer = self.parse_filename_metadata(filename)

                found_files.append(
                    {
                        "carbon_number": carbon_num,
                        "url": xyz_url,
                        "filename": filename,
                        "discovery_method": "page_source",
                        "symmetry": symmetry,
                        "isomer": isomer,
                    }
                )
                self.logger.info(f"Found XYZ link in page source: {xyz_url}")

        except Exception as e:
            self.logger.error(f"Error extracting links for C{carbon_num}: {e}")

        # Remove duplicates
        unique_files = []
        seen_urls = set()
        for file_info in found_files:
            if file_info["url"] not in seen_urls:
                seen_urls.add(file_info["url"])
                unique_files.append(file_info)

        self.logger.info(
            f"Found {len(unique_files)} unique XYZ files for C{carbon_num}"
        )
        return unique_files

    def parse_filename_metadata(self, filename: str) -> tuple:
        """Parse symmetry and isomer information from filename"""
        # Try pattern: C{num}-{symmetry}-{isomer}.xyz
        match = re.match(r"C\d+-(.+)-(\d+)\.xyz", filename)
        if match:
            return match.group(1), int(match.group(2))

        # Try pattern: C{num}-{isomer}.xyz
        match = re.match(r"C\d+-(\d+)\.xyz", filename)
        if match:
            return "unknown", int(match.group(1))

        return "unknown", 0

    def create_molecule_dictionary(self) -> Dict[str, Dict[str, str]]:
        """
        Create a dictionary with molecule names as keys and download info as values
        Format: "C80_D5d": {"description": "...", "download_link": "..."}
        Only keeps the first structure found for each carbon number + symmetry combination
        """
        molecule_dict = {}
        seen_combinations = set()  # Track (carbon_number, symmetry) combinations

        # Sort files by carbon number and isomer to get consistent "first" entries
        sorted_files = sorted(
            self.xyz_files, key=lambda x: (x["carbon_number"], x.get("isomer", 0))
        )

        for file_info in sorted_files:
            carbon_num = file_info["carbon_number"]
            symmetry = file_info.get("symmetry", "unknown")

            # Clean up symmetry notation
            if symmetry == "unknown" or symmetry == "":
                symmetry = None

            # Create unique combination key
            combination_key = (carbon_num, symmetry)

            # Skip if we've already seen this carbon number + symmetry combination
            if combination_key in seen_combinations:
                self.logger.debug(
                    f"Skipping duplicate C{carbon_num} with symmetry {symmetry}"
                )
                continue

            # Mark this combination as seen
            seen_combinations.add(combination_key)

            # Determine molecule name
            if symmetry and symmetry != "unknown":
                molecule_name = f"C{carbon_num}_{symmetry}"
                description = f"Download link for C{carbon_num} fullerene with point group symmetry {symmetry}"
            else:
                molecule_name = f"C{carbon_num}"
                description = f"Download link for C{carbon_num} fullerene"

            molecule_dict[molecule_name] = {
                "description": description,
                "download_link": file_info["url"],
            }

            self.logger.info(f"Added to dictionary: {molecule_name}")

        return molecule_dict

    def verify_xyz_files(self, max_verify: int = 10) -> int:
        """
        Verify that found XYZ files are actually downloadable
        """
        self.logger.info(f"Verifying up to {max_verify} XYZ files...")
        verified_count = 0

        for i, file_info in enumerate(self.xyz_files[:max_verify]):
            try:
                response = self.session.head(file_info["url"], timeout=10)
                if response.status_code == 200:
                    file_info["verified"] = True
                    file_info["size"] = response.headers.get(
                        "content-length", "unknown"
                    )
                    verified_count += 1
                    self.logger.info(f"Verified: {file_info['filename']}")
                else:
                    file_info["verified"] = False
                    self.logger.warning(
                        f"Could not verify: {file_info['filename']} (status: {response.status_code})"
                    )

                time.sleep(0.5)

            except Exception as e:
                file_info["verified"] = False
                self.logger.warning(f"Error verifying {file_info['filename']}: {e}")

        self.logger.info(
            f"Verified {verified_count}/{min(max_verify, len(self.xyz_files))} files"
        )
        return verified_count

    def scrape_all_files(self) -> List[Dict]:
        """
        Main method to scrape all XYZ files using Selenium
        """
        self.logger.info("Starting Selenium-based fullerene XYZ file scraping...")

        try:
            # Initialize Selenium driver
            self.driver = self.setup_driver()

            # Discover available carbon numbers
            carbon_numbers = self.discover_carbon_numbers()

            if not carbon_numbers:
                self.logger.error(
                    "No carbon numbers discovered. Check website structure."
                )
                return []

            # Extract XYZ files for each carbon number
            for carbon_num in sorted(carbon_numbers):
                try:
                    found_files = self.extract_xyz_links_from_page(carbon_num)
                    self.xyz_files.extend(found_files)

                    # Brief pause between pages
                    time.sleep(2)

                except Exception as e:
                    self.logger.error(f"Error processing C{carbon_num}: {e}")
                    continue

            # Verify some of the found files
            if self.xyz_files:
                self.verify_xyz_files(max_verify=5)

            self.logger.info(
                f"Scraping complete! Found {len(self.xyz_files)} XYZ files total."
            )

        except Exception as e:
            self.logger.error(f"Fatal error during scraping: {e}")

        finally:
            # Always close the driver
            if self.driver:
                self.driver.quit()
                self.logger.info("Browser closed")

        return self.xyz_files

    def save_results(self):
        """Save results to various output formats with the requested JSON dictionary format"""
        self.logger.info("Saving results...")

        # Create molecule dictionary in the requested format
        molecule_dict = self.create_molecule_dictionary()

        # Save as formatted JSON dictionary (main output)
        json_file = self.output_dir / "fullerene_molecules.json"
        with open(json_file, "w") as f:
            json.dump(molecule_dict, f, indent=4, sort_keys=True)

        # Save raw data as JSON for reference
        raw_json_file = self.output_dir / "selenium_fullerene_xyz_files_raw.json"
        with open(raw_json_file, "w") as f:
            json.dump(self.xyz_files, f, indent=2)

        # Save as CSV
        csv_file = self.output_dir / "selenium_fullerene_xyz_files.csv"
        if self.xyz_files:
            with open(csv_file, "w", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=self.xyz_files[0].keys())
                writer.writeheader()
                writer.writerows(self.xyz_files)

        # Save as plain text URL list
        txt_file = self.output_dir / "selenium_fullerene_xyz_urls.txt"
        with open(txt_file, "w") as f:
            for file_info in sorted(
                self.xyz_files, key=lambda x: (x["carbon_number"], x.get("isomer", 0))
            ):
                f.write(f"{file_info['url']}\n")

        # Create summary
        summary_file = self.output_dir / "selenium_summary.txt"
        with open(summary_file, "w") as f:
            f.write(f"Selenium Fullerene XYZ Files Scraping Summary\n")
            f.write(f"===========================================\n\n")
            f.write(f"Total XYZ files found: {len(self.xyz_files)}\n")
            f.write(f"Unique molecules in dictionary: {len(molecule_dict)}\n")
            f.write(f"Carbon numbers available: {sorted(self.carbon_numbers)}\n\n")

            # Group by carbon number
            by_carbon = {}
            for file_info in self.xyz_files:
                carbon = file_info["carbon_number"]
                if carbon not in by_carbon:
                    by_carbon[carbon] = []
                by_carbon[carbon].append(file_info)

            f.write("Files per carbon number:\n")
            for carbon in sorted(by_carbon.keys()):
                verified = sum(1 for f in by_carbon[carbon] if f.get("verified", False))
                f.write(
                    f"  C{carbon}: {len(by_carbon[carbon])} files ({verified} verified)\n"
                )

            f.write("\nDiscovery methods used:\n")
            methods = {}
            for file_info in self.xyz_files:
                method = file_info.get("discovery_method", "unknown")
                methods[method] = methods.get(method, 0) + 1

            for method, count in methods.items():
                f.write(f"  {method}: {count} files\n")

            f.write(f"\nMolecule dictionary sample:\n")
            for i, (molecule_name, info) in enumerate(
                sorted(molecule_dict.items())[:5]
            ):
                f.write(f"  {molecule_name}: {info['download_link']}\n")

        self.logger.info(f"Results saved to {self.output_dir}/")
        self.logger.info(f"Main result file: {json_file}")
        self.logger.info(f"Molecule dictionary contains {len(molecule_dict)} entries")

    def download_sample_files(self, max_downloads: int = 3):
        """Download a few sample XYZ files to verify they work"""
        self.logger.info(f"Downloading up to {max_downloads} sample files...")

        download_dir = self.output_dir / "sample_xyz_files"
        download_dir.mkdir(exist_ok=True)

        downloaded = 0
        for file_info in self.xyz_files[:max_downloads]:
            try:
                response = self.session.get(file_info["url"], timeout=30)
                response.raise_for_status()

                local_file = download_dir / file_info["filename"]
                with open(local_file, "wb") as f:
                    f.write(response.content)

                self.logger.info(
                    f"Downloaded: {file_info['filename']} ({len(response.content)} bytes)"
                )
                downloaded += 1

                time.sleep(1)

            except Exception as e:
                self.logger.error(f"Failed to download {file_info['url']}: {e}")

        self.logger.info(f"Downloaded {downloaded} sample files to {download_dir}/")


def main():
    """Main function to run the Selenium scraper"""
    print("Selenium Fullerene XYZ File Scraper")
    print("===================================")
    print("This script requires ChromeDriver to be installed.")
    print("Download from: https://chromedriver.chromium.org/")
    print()

    # You can specify ChromeDriver path here if it's not in PATH
    chromedriver_path = None  # e.g., "/path/to/chromedriver" or "C:\\chromedriver.exe"

    scraper = SeleniumFullereneScraper(
        headless=True,  # Set to False to see the browser in action
        chromedriver_path=chromedriver_path,
    )

    try:
        # Run the scraping
        scraper.scrape_all_files()

        # Save results
        scraper.save_results()

        # Create and display molecule dictionary
        molecule_dict = scraper.create_molecule_dictionary()

        print(f"\n✅ Scraping completed successfully!")
        print(f"📊 Found {len(scraper.xyz_files)} XYZ files")
        print(f"🎯 Created dictionary with {len(molecule_dict)} unique molecules")
        print(f"📁 Results saved to: {scraper.output_dir}/")
        print(f"📋 Main result: fullerene_molecules.json")

        if molecule_dict:
            print(f"\n📋 Sample molecules found:")
            for i, (molecule_name, info) in enumerate(
                sorted(molecule_dict.items())[:5]
            ):
                print(f"  {i+1}. {molecule_name}")
            if len(molecule_dict) > 5:
                print(f"  ... and {len(molecule_dict) - 5} more molecules")

        # Only download samples if explicitly requested
        download_samples = (
            input(
                "\nDo you want to download sample XYZ files for verification? (y/N): "
            )
            .lower()
            .strip()
        )
        if download_samples in ["y", "yes"]:
            if scraper.xyz_files:
                # First verify some files
                scraper.verify_xyz_files(max_verify=5)
                # Then download samples
                scraper.download_sample_files(max_downloads=3)
            print("Sample files downloaded to sample_xyz_files/ directory")

        print(
            f"\n💡 Your JSON dictionary is ready at: {scraper.output_dir}/fullerene_molecules.json"
        )
        print(
            "Use the download links in the JSON file to download specific XYZ files as needed."
        )

    except KeyboardInterrupt:
        print("\n❌ Scraping interrupted by user")
    except Exception as e:
        print(f"\n❌ Error during scraping: {e}")
        scraper.logger.error(f"Fatal error: {e}")


if __name__ == "__main__":
    main()
